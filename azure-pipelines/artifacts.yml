# Artifact generation pipeline
#
# For general remarks, see ../README.md

schedules:
  - cron: "0 */6 * * *" # Every 6 hours
    displayName: Periodic build
    branches:
      include:
        - master

trigger: none

pr: none

variables:
  SLACK_HOOK: ***REMOVED***

stages:
  - stage: makeArtifacts
    dependsOn: []
    displayName: Make all artifacts
    pool:
      name: Native
    jobs:
      - job:
        displayName: make all-js
        steps:
          - bash: |
              set -e
              build/bin/install-vault.sh
              make all-js
            env:
              # Map secrets to env vars. This needs to be done manually
              AWS_ACCESS_KEY_ID: $(SECRET_AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(SECRET_AWS_SECRET_ACCESS_KEY)
              BASH_ENV: "~/.bashrc"
          - publish: $(System.DefaultWorkingDirectory)/dist/js
            artifact: js-packages
            condition: succeeded()
      - job:
        displayName: make all-windows
        steps:
          - bash: |
              set -e
              build/bin/install-vault.sh
              chmod -R 777 .
              chmod -R 777 /root/.cargo
              make all-windows
            env:
              # Map secrets to env vars. This needs to be done manually
              AWS_ACCESS_KEY_ID: $(SECRET_AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(SECRET_AWS_SECRET_ACCESS_KEY)
              BASH_ENV: "~/.bashrc"
          - publish: $(System.DefaultWorkingDirectory)/dist/bin
            artifact: windows-binaries
            condition: succeeded()
      - job: makeAllLinux
        displayName: make all-linux
        steps:
          - bash: |
              set -e
              build/bin/install-vault.sh
              chmod -R 777 .
              chmod -R 777 /root/.cargo
              make all-linux
            env:
              # Map secrets to env vars. This needs to be done manually
              AWS_ACCESS_KEY_ID: $(SECRET_AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(SECRET_AWS_SECRET_ACCESS_KEY)
              BASH_ENV: "~/.bashrc"
          - publish: $(System.DefaultWorkingDirectory)/dist/bin
            artifact: linux-binaries
            condition: succeeded()
      - job: makeActyxDocker
        displayName: actyx multiarch docker images
        steps:
          - bash: |
              set -e
              GIT_COMMIT=`git log -1 --pretty=%H`
              MASTER_TAG=
              head_of_master=`git rev-parse origin/master`
              MASTER_TAG=""
              if [ "$GIT_COMMIT" == "$head_of_master" ]; then
                  echo "Running on HEAD of master ($head_of_master), tagging image as latest"
                  MASTER_TAG="--tag actyx/cosmos:actyx-latest"
              fi
              docker buildx build \
                --push \
                --platform linux/arm64/v8,linux/amd64,linux/arm/v6,linux/arm/v7 \
                --build-arg GIT_COMMIT=$GIT_COMMIT \
                --tag actyx/cosmos:actyx-$GIT_COMMIT $MASTER_TAG \
                -f ops/docker/images/actyx/Dockerfile \
                .
            env:
              BASH_ENV: "~/.bashrc"
      - job:
        displayName: make all-android
        steps:
          - bash: |
              set -e
              build/bin/install-vault.sh
              chmod -R 777 .
              chmod -R 777 /root/.cargo
              make all-android
            env:
              # Map secrets to env vars. This needs to be done manually
              AWS_ACCESS_KEY_ID: $(SECRET_AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(SECRET_AWS_SECRET_ACCESS_KEY)
              BASH_ENV: "~/.bashrc"
          - publish: $(System.DefaultWorkingDirectory)/dist/bin
            artifact: android-binaries
            condition: succeeded()

  - stage: upload
    displayName: Upload to az blob store
    dependsOn:
      - makeArtifacts
    jobs:
      - job:
        steps:
          - task: DownloadPipelineArtifact@2
            inputs:
              path: pipeline-artifacts

          - task: AzureCLI@2
            # Now, there would be the nicely looking AzureFileCopy@4 task, which would use the `azcopy` cli (yes, it's not
            # included in `az`), to upload a bunch of files:
            # https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/deploy/azure-file-copy?view=azure-devops.
            #
            # Now, that task currently only works under Windows.
            # Next up: Directly use `azcopy`. No built-in authentication via azure pipelines, bummer.
            # Luckily, the `az` cli offers a legacy command `storage  blob upload-batch`.
            displayName: Upload
            inputs:
              azureSubscription: azure-ax-ci
              scriptType: bash
              scriptLocation: inlineScript
              inlineScript: |
                set -e
                az --version
                az account show
                az storage blob upload-batch -s $(System.DefaultWorkingDirectory)/pipeline-artifacts -d https://axartifacts.blob.core.windows.net/artifacts/ --destination-path $(Build.SourceVersion)/

  - stage: notifyMakeSuccess
    displayName: Notify Slack on successful make all
    dependsOn:
      - upload
      - integration
      - release
    jobs:
      - template: jobs-notify-success.yml

  - stage: notifyMakeFailure
    displayName: Notify Slack on failure
    condition: failed()
    dependsOn:
      - makeArtifacts
      - upload
      - integration
      - release
    jobs:
      - template: slack-notify-failure.yml

  - stage: integration
    displayName: Nightly integration test
    pool:
      name: Native
    dependsOn:
      - upload
    jobs:
      # Integration tests are disabled for the moment, but will be enabled
      # again once the banyan integration reaches a certain stability.
      #      - job: integrationTest
      #        displayName: Integration Test
      #        steps:
      #          - task: DownloadPipelineArtifact@2
      #            inputs:
      #              path: pipeline-artifacts
      #          - bash: |
      #              set -e
      #              mkdir -p dist/bin
      #              cd pipeline-artifacts
      #              for i in `ls`; do mv $i/* ../dist/bin/; done
      #              cd ../dist
      #              find -type f -exec chmod +x {} \;
      #              cp -r bin/linux-x86_64 bin/current
      #            displayName: Massage artifact directory structure
      #            env:
      #              BASH_ENV: "~/.bashrc"
      #          - bash: |
      #              set -e
      #              build/bin/install-vault.sh
      #              make prepare-js all-js
      #              cd integration
      #              source ~/.nvm/nvm.sh
      #              nvm use
      #              npm install
      #              npm test
      #            env:
      #              # Map secrets to env vars. This needs to be done manually
      #              AWS_ACCESS_KEY_ID: $(SECRET_AWS_ACCESS_KEY_ID)
      #              AWS_SECRET_ACCESS_KEY: $(SECRET_AWS_SECRET_ACCESS_KEY)
      #              BASH_ENV: "~/.bashrc"
      #            displayName: npm test
      #          - publish: $(System.DefaultWorkingDirectory)/integration/logs
      #            condition: failed()
      #            artifact: integration-test-logs-attempt$(System.JobAttempt)
      - job: cleanup
        displayName: Integration test cleanup tasks
        condition: always()
        steps:
          - bash: |
              set -e
              source ~/.nvm/nvm.sh
              cd integration
              nvm use
              npm install
              npm run cleanup 43200
            env:
              # Map secrets to env vars. This needs to be done manually
              AWS_ACCESS_KEY_ID: $(SECRET_AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(SECRET_AWS_SECRET_ACCESS_KEY)
              BASH_ENV: "~/.bashrc"
            name: cleanupInstances
            displayName: Clean up old integration test instances
  - stage: release
    displayName: Continuous release
    pool:
      name: Native
    dependsOn:
      - upload # If no artifacts have been built this shouldn't work
      - integration # Same if the integration tests didn't succeed
    jobs:
      - job: release
        displayName: Release
        condition: eq(variables['Build.SourceBranch'], 'refs/heads/master')
        steps:
          - checkout: self # self represents the repo where the initial Pipelines YAML file was found
            clean: true # whether to fetch clean each time
            submodules: recursive # set to 'true' for a single level of submodules or 'recursive' to get submodules of submodules
            persistCredentials: true # set to 'true' to leave the OAuth token in the Git config after the initial fetch
          - bash: |
              set -e
              cd release
              nvm use
              npm install
              echo "Building cosmos-release"
              npm run build
              echo "Running release"
              echo "Repo dir: $REPO_DIR"
              echo "Target rev: $TARGET_REV"
              echo "Setting git email and username"
              git config user.email "cosmos-release@actyx.io"
              git config user.name "cosmos-release"
              echo "Pruning local tags"
              git fetch --force --prune origin "+refs/tags/*:refs/tags/*"
              echo "Getting tags from remote"
              git fetch --all --tags
              npm run cli -- release 1 $TARGET_REV $ARM_NF --localRepo $REPO_DIR --skipMalformed --skipIfNoNotes
            env:
              ARM_NF: $(RELEASE_NETLIFY_ACCESS_TOKEN)
              REPO_DIR: $(Build.SourcesDirectory)
              TARGET_REV: $(Build.SourceVersion)
              BASH_ENV: "~/.bashrc"
            displayName: run release
