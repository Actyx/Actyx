# This file configures how azure pipelines are being run on our two CI boxes ci-0.actyx.net and ci-2.actyx.net
#
# CI jobs run on these machines as user ubuntu.
# Tools and headers that are needed for the build can be added by logging into both CI servers and installing them
# using `sudo apt install`.
# Currently used tools that are not installed during make prepare are: jq, protoc, libssl-dev, build-essential.
#
# Each machine provides a number of azure pipelines workers, currently 12. These can be accessed with the pool Native.
#
# The workers are started as systemd services, configured in `/etc/systemd/system/vsts-agent.*.service`
# The working directory is a subdirectory of `/ubuntu/home`, e.g. `/ubunutu/home/agents/agent-0`
# Each job gets a numbered directory in `_work/`, e.g. `/home/ubuntu/agents/agent-0/_work/4`
# Source is checked out into a subdirectory called s, e.g. `/home/ubuntu/agents/agent-0/_work/4/s`
# The current working directory while exexuting jobs is this directory.
#
# On startup, each agent can be configured by some files in the agent directory, `/home/ubuntu/agents/agent-0/runsvc.sh`
#
# Environment variables are configured in the azure pipeline GUI. Environment variables marked as secret must be
# explicitly imported into jobs in the env section. E.g. AWS_ACCESS_KEY_ID: $(SECRET_AWS_ACCESS_KEY_ID)
#
# NOTE: We rely on the host to periodically do `docker login` both to DockerHub and GitHub Packages instead of relying
# on Azure Pipelines to do it, since having Pipelines do it introduces a race condition where, if a job fails,
# it logs out from DockerHub. Since we're using host-based agents, this logs out every single running pipeline,
# leading to random failures. All CI hosts have a crontab that logs in to both every hour.
# See https://github.com/Actyx/Cosmos/pull/5138.

schedules:
  - cron: "0 0 * * *"
    displayName: Periodic build
    branches:
      include:
        - master

variables:
  SLACK_HOOK: ***REMOVED***

stages:
  - stage: makeArtifacts
    dependsOn: []
    displayName: Make all artifacts
    pool:
      name: Native
    jobs:
      - job:
        displayName: make all-js
        steps:
          - bash: |
              build/bin/install-vault.sh
              make all-js
            env:
              # Map secrets to env vars. This needs to be done manually
              AWS_ACCESS_KEY_ID: $(SECRET_AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(SECRET_AWS_SECRET_ACCESS_KEY)
          - publish: $(System.DefaultWorkingDirectory)/dist/js
            artifact: js-packages
            condition: succeeded()
      - job:
        displayName: make all-windows
        steps:
          - bash: |
              build/bin/install-vault.sh
              make all-windows
            env:
              # Map secrets to env vars. This needs to be done manually
              AWS_ACCESS_KEY_ID: $(SECRET_AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(SECRET_AWS_SECRET_ACCESS_KEY)
          - publish: $(System.DefaultWorkingDirectory)/dist/bin
            artifact: windows-binaries
            condition: succeeded()
      - job:
        displayName: make all-linux
        steps:
          - bash: |
              build/bin/install-vault.sh
              make all-linux
            env:
              # Map secrets to env vars. This needs to be done manually
              AWS_ACCESS_KEY_ID: $(SECRET_AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(SECRET_AWS_SECRET_ACCESS_KEY)
          - publish: $(System.DefaultWorkingDirectory)/dist/bin
            artifact: linux-binaries
            condition: succeeded()
      - job:
        displayName: make all-android
        steps:
          - bash: |
              build/bin/install-vault.sh
              make all-android
            env:
              # Map secrets to env vars. This needs to be done manually
              AWS_ACCESS_KEY_ID: $(SECRET_AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(SECRET_AWS_SECRET_ACCESS_KEY)
          - publish: $(System.DefaultWorkingDirectory)/dist/bin
            artifact: android-binaries
            condition: succeeded()
  #  TODO
  # - stage: makeContainers
    # displayName: make containers


  - stage: upload
    displayName: Upload to az blob store
    jobs:
      - job:
        steps:
          - task: DownloadPipelineArtifact@2
            inputs:
              path: pipeline-artifacts
            

          - task: AzureCLI@2
            # No, there would be the nicely looking AzureFileCopy@4 task, which would use the `azcopy` cli (yes, it's not
            # included in `az`), to upload a bunch of files:
            # https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/deploy/azure-file-copy?view=azure-devops.
            #
            # Now, that task currently only works under Windows.
            # Next up: Directly use `azcopy`. No built-in authentication via azure pipelines, bummer.
            # Luckily, the `az` cli offers a legacy command `storage  blob upload-batch`.
            displayName: Upload
            inputs:
              azureSubscription: azure-ax-ci
              scriptType: bash
              scriptLocation: inlineScript
              inlineScript: |
                az --version
                az account show
                az storage blob upload-batch -s $(System.DefaultWorkingDirectory)/pipeline-artifacts -d https://axartifacts.blob.core.windows.net/artifacts/ --destination-path $(Build.SourceVersion)/

  - stage: notifyMakeSuccess
    dependsOn:
      - makeArtifacts
    displayName: Notify Slack on successful make all
    jobs:
      - template: jobs-notify-success.yml

  - stage: integration
    displayName: Nightly integration test
    jobs:
      - job: cleanup
        displayName: Integration test cleanup tasks
        steps:
          - bash: |
              cd integration
              # TODO NVM?
              npm install
              npm run cleanup 43200
            env:
              # Map secrets to env vars. This needs to be done manually
              AWS_ACCESS_KEY_ID: $(SECRET_AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(SECRET_AWS_SECRET_ACCESS_KEY)
            name: cleanupInstances
            displayName: Clean up old integration test instances